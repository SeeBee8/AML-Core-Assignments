{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a37689e-2907-4007-b75f-4dad1d809397",
   "metadata": {},
   "source": [
    "# Comparing Texts (Core)\n",
    "\n",
    "*Christina Brockway*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4bd2ce-c162-4055-a6e9-0fbfb0a83c97",
   "metadata": {},
   "source": [
    "Using:  https://www.gnu.org/licenses/agpl-3.0.html\n",
    "downloaded from Kaggle\n",
    "\n",
    "This data set contains labeled real and fake news articles from around the 2017 US presidential elections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80215de-1219-43d0-a7e4-78022f3d28e4",
   "metadata": {},
   "source": [
    "Once you have loaded the data into a dataframe:\r\n",
    "1.  \r\n",
    "Clean the data. Remove any unnecessary columns and check for/remove duplicate\n",
    "2.  .\r\n",
    "Prepare the data. Create 3 new colum\n",
    "    1.  s:\r\n",
    "Tokenized texts: just split the texts, don't remove stop words or punctua\n",
    "    2.  ion\r\n",
    "Lemmatized texts: remove stopwords, and punctuation, and lemmatize the w        -  ords\r\n",
    "IMPORTANT! When you load in the SpaCy NLP object, remember to disable the parser and named object recognizer using the following: spacy.load('en_core_web_sm', disable=['parser', '\n",
    "    3.  er'])\r\n",
    "Joined lemmatiz\n",
    "        -  d data\r\n",
    "join each lemmatized document into a single\n",
    "4.  string.\r\n",
    "Analyze class balance and document\n",
    "    1.   lengths:\r\n",
    "What is the class balance? How many real and fake articles \n",
    "    2.   .  re there?\r\n",
    "What is the average word count for real news articles? What abou\n",
    "    3.   fake ones?\r\n",
    "Hint, you can map the len() function to the tokenized text to create a new column, then find the average of \n",
    "5.  hat column.\r\n",
    "Compare the word\n",
    "    1.  frequencies:\r\n",
    "Create and plot the frequency distribution plots for the 20 most common words in real and fake news articles. (\n",
    "        -   total plots)\r\n",
    "Use the \n",
    "    3.  emmatized text\r\n",
    "Create word clouds for each of the article types, real and fake (2 to\n",
    "        -  al word clouds)\r\n",
    "Use the joined lemmatized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e72309b-9b60-4d07-8d66-c68a7abc8217",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3064150359.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[27], line 8\u001b[1;36m\u001b[0m\n\u001b[1;33m    download('en_core_web_sm') as nlp\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import STOPWORDS\n",
    "from nltk.probability import FreqDist\n",
    "import nltk\n",
    "import spacy \n",
    "from spacy.cli import download\n",
    "download('en_core_web_sm') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba71248-88cf-4248-8bc1-3ebbc9a39265",
   "metadata": {},
   "source": [
    "## 1.  Clean the Data\n",
    "Remove any unnecessary columns and check for/remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b06b276e-88df-4d48-abb8-ae9d46a17032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A whirlwind day in D.C. showcases Trump’s unor...</td>\n",
       "      <td>Donald Trump endorsed an unabashedly noninterv...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In Baltimore's call for federal police probe, ...</td>\n",
       "      <td>While some Justice Department investigations a...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0  A whirlwind day in D.C. showcases Trump’s unor...   \n",
       "1           1  In Baltimore's call for federal police probe, ...   \n",
       "\n",
       "                                                text label  \n",
       "0  Donald Trump endorsed an unabashedly noninterv...  REAL  \n",
       "1  While some Justice Department investigations a...  REAL  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/Fake_Real_News_Data.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b6188f5-fca7-43d9-996f-c7a1d48c71f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop(columns='Unnamed: 0', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a206eff-a4f5-4f51-b23e-cae41922ee74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34ec9865-c0f4-4400-b533-3097bbdd0c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79ab4b85-bc2d-47a1-a3f4-e7c217dff57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60fce299-8b45-4fe3-8158-525435f7298f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title    0\n",
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aeabfe-801f-4ae5-81e1-977204f8eede",
   "metadata": {},
   "source": [
    "## 2. Prepare the Data\n",
    "- Create 3 new Columns\n",
    "    1.  Tokenized texts\n",
    "    2.  Lemmatized textes\n",
    "    3.  Joined lemmatized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e991d25-3bd3-46fe-8be4-a042aba4409a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1a6f5e269e0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f508403e-4abb-44b5-a276-4b41dc43c1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Steps in the pipeline\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd7cf222-429c-4de5-b575-c23acd611886",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create dictionary for desired attributes for each token\u001b[39;00m\n\u001b[0;32m      2\u001b[0m token_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdoc\u001b[49m:\n\u001b[0;32m      4\u001b[0m     token_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.text\u001b[39m\u001b[38;5;124m\"\u001b[39m: token\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.lemma_\u001b[39m\u001b[38;5;124m\"\u001b[39m: token\u001b[38;5;241m.\u001b[39mlemma_,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.is_space\u001b[39m\u001b[38;5;124m\"\u001b[39m: token\u001b[38;5;241m.\u001b[39mis_space\n\u001b[0;32m     11\u001b[0m     }\n\u001b[0;32m     12\u001b[0m     token_data\u001b[38;5;241m.\u001b[39mappend(token_dict)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "#slicing tokens from the doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb45297-0ecd-4bfe-afc6-8852a5a80295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea46cb66-1a01-44db-811e-dee64d16f20a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d8bb86-6a9e-43e6-9cd0-decdee9a40d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
